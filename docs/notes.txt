We want to decouple representation with computation

std::reference_wrapper<Tensor> vs Tensor* ? 

Future Tensor ideas:
    
    std::vector<Tensor*> source; // dependencies of current Tensor
    Op op; // operation that formed this Tensor


Should buffers be completely decoupled from Tensors?
    idea here may be to have pool of buffers, build tensor/compute dependencies,
    then select which buffers to perform compute on. (explore this later...)


must support bf16


How to make git run gtest tests on push / commit? Is this possible?

smart pointer to wrap view objects? currently no user defined destructor, could simplify copy/move

clean up tensor::eye and write test for it



M3 supports 128-bit SIMD (NEON)
x86 supports up to 512-bit SIMD (AVX)





IDEA: perhaps we don't attach tensor on construction, further decoupling between tensor and buffer allow
us to keep the tensor lighter and only bufferize when we need to do computation or print results


how to make buffer compatable with STL algorithms or ranges? 
    Iterators? what else?


come back to this when implementing manual SIMD -> 
class buffer {
    constexpr static std::size_t align_val{ 32 };
    constexpr static std::align_val_t alignment{ align_val };

    [[nodiscard]] T* allocate(std::size_t n) {
        if (n > max_size() || n <= 0) throw std::bad_array_new_length();
        capacity = ((n + align_val - 1) & ~(align_val - 1)); // round to nearest multiple of 32
        ptr = static_cast<T*>(::operator new(capacity * sizeof(T), alignment));
        if (!ptr) throw std::bad_alloc();
        return ptr;
    }
}